# Copyright 2024 Humanitec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
  
# The default volume provisioner provided by score-compose allows basic volume resources to be created in the resources
# system. The volume resource just creates an ephemeral Docker volume with a random string as the name, and source
# attribute that we can reference.
- uri: template://default-provisioners/volume
  # By default, match all classes and ids of volume. If you want to override this, create another provisioner definition
  # with a higher priority.
  type: volume
  init: |
    randomVolumeName: {{ .Id | replace "." "-" }}-{{ randAlphaNum 6 }}
  # Store the random volume name if we haven't chosen one yet, otherwise use the one that exists already
  state: |
    name: {{ dig "name" .Init.randomVolumeName .State }}
  # Return a source value with the volume name. This can be used in volume resource references now.
  outputs: |
    source: {{ .State.name }}
  # Add a volume to the docker compose file. We assume our name is unique here. We also apply a label to help ensure
  # that we can track the volume back to the workload and resource that created it.
  volumes: |
    {{ .State.name }}:
      name: {{ .State.name }}
      driver: local
      labels:
        dev.score.compose.res.uid: {{ .Uid }}

# The default provisioner for service resources, this expects a workload and port name and will return the hostname and
# port required to contact it. This will validate that the workload and port exist, but won't enforce a dependency
# relationship yet.
- uri: template://default-provisioners/service-port
  type: service-port
  class: default
  outputs: |
    {{ if not .Params.workload }}{{ fail "expected 'workload' param for the target workload name" }}{{ end }}
    {{ if not .Params.port }}{{ fail "expected 'port' param for the name of the target workload service port" }}{{ end }}
    {{ $w := (index .WorkloadServices .Params.workload) }}
    {{ if not $w }}{{ fail "unknown workload" }}{{ end }}
    {{ $p := (index $w.Ports .Params.port) }}
    {{ if not $p }}{{ fail "unknown service port" }}{{ end }}
    hostname: {{ $w.ServiceName | quote }}
    port: {{ $p.TargetPort }}

# The default redis provisioner adds a redis service to the project which returns a host, port, username, and password.
- uri: template://default-provisioners/redis
  # By default, match all redis types regardless of class and id. If you want to override this, create another
  # provisioner definition with a higher priority.
  type: redis
  # Init template has the default port and a random service name and password if needed later
  init: |
    port: 6379
    randomServiceName: redis-{{ randAlphaNum 6 }}
    randomPassword: {{ randAlphaNum 16 | quote }}
  # The only state we need to persist is the chosen random service name and password
  state: |
    serviceName: {{ dig "serviceName" .Init.randomServiceName .State | quote }}
    password: {{ dig "password" .Init.randomPassword .State | quote }}
  # Return the outputs schema that consumers expect
  outputs: |
    host: {{ .State.serviceName }}
    port: {{ .Init.port }}
    username: default
    password: {{ .State.password | quote }}
  # write the config file to the mounts directory
  files: |
    {{ .State.serviceName }}/redis.conf: |
      requirepass {{ .State.password }}
      port {{ .Init.port }}
      save 60 1
      loglevel warning
  # add a volume for persistence of the redis data
  volumes: |
    {{ .State.serviceName }}-data:
      name: {{ .State.serviceName }}-data
      driver: local
      labels:
        dev.score.compose.res.uid: {{ .Uid }}
  # And the redis service itself with volumes bound in
  services: |
    {{ .State.serviceName }}:
      labels:
        dev.score.compose.res.uid: {{ .Uid }}
      image: redis:7
      restart: always
      entrypoint: ["redis-server"]
      command: ["/usr/local/etc/redis/redis.conf"]
      volumes:
      - type: bind
        source: {{ .MountsDirectory }}/{{ .State.serviceName }}/redis.conf
        target: /usr/local/etc/redis/redis.conf
        read_only: true
      - type: volume
        source: {{ .State.serviceName }}-data
        target: /data
        volume:
          nocopy: true
  info_logs: |
    - "To connect to redis: \"docker run -it --network {{ .ComposeProjectName }}_default --rm redis redis-cli -h {{ .State.serviceName | squote }} -a {{ .State.password | squote }}\""

# The default postgres provisioner adds a postgres instance and then ensures that the required databases are created on
# startup.
- uri: template://default-provisioners/postgres
  # By default, match all redis types regardless of class and id. If you want to override this, create another
  # provisioner definition with a higher priority.
  type: postgres
  # Init template has the random service name and password if needed later
  init: |
    randomServiceName: pg-{{ randAlphaNum 6 }}
    randomDatabase: db-{{ randAlpha 8 }}
    randomUsername: user-{{ randAlpha 8 }}
    randomPassword: {{ randAlphaNum 16 | quote }}
    sk: default-provisioners-postgres-instance
    publishPort: {{ dig "annotations" "compose.score.dev/publish-port" "0" .Metadata | quote }}
  # The state for each database resource is a unique db name and credentials
  state: |
    database: {{ dig "database" .Init.randomDatabase .State | quote }}
    username: {{ dig "username" .Init.randomUsername .State | quote }}
    password: {{ dig "password" .Init.randomPassword .State | quote }}
  # All instances agree on the shared state since there is no concurrency here
  shared: |
    {{ .Init.sk }}:
      instanceServiceName: {{ dig .Init.sk "instanceServiceName" .Init.randomServiceName .Shared | quote }}
      instancePassword: {{ dig .Init.sk "instancePassword" .Init.randomPassword .Shared | quote }}
  # The outputs are the core database outputs. We output both name and database for broader compatibility.
  outputs: |
    host: {{ dig .Init.sk "instanceServiceName" "" .Shared }}
    port: 5432
    name: {{ .State.database }}
    database: {{ .State.database }}
    username: {{ .State.username }}
    password: {{ .State.password }}
  # Write out an idempotent create script per database
  files: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-db-scripts/{{ .State.database }}.sql: |
      SELECT 'CREATE DATABASE "{{ .State.database }}"' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = '{{ .State.database }}')\gexec
      SELECT $$CREATE USER "{{ .State.username }}" WITH PASSWORD '{{ .State.password }}'$$ WHERE NOT EXISTS (SELECT FROM pg_roles WHERE rolname = '{{ .State.username }}')\gexec
      GRANT ALL PRIVILEGES ON DATABASE "{{ .State.database }}" TO "{{ .State.username }}";
  # Ensure the data volume exists
  volumes: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-data:
      driver: local
  # Create 2 services, the first is the database itself, the second is the init container which runs the scripts
  services: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}:
      image: postgres:16-alpine
      restart: always
      environment:
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{ dig .Init.sk "instancePassword" "" .Shared | quote }}
      {{ if ne .Init.publishPort "0" }}
      ports:
      - target: 5432
        published: {{ .Init.publishPort }}
      {{ end }}
      volumes:
      - type: volume
        source: {{ dig .Init.sk "instanceServiceName" "" .Shared }}-data
        target: /var/lib/postgresql/data
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "postgres"]
        interval: 2s
        timeout: 2s
        retries: 10
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-init:
      image: postgres:16-alpine
      entrypoint: ["/bin/sh"]
      environment:
        POSTGRES_PASSWORD: {{ dig .Init.sk "instancePassword" "" .Shared | quote }}
      command:
      - "-c"
      - |
        cd /db-scripts
        ls db-*.sql | xargs cat | psql "postgresql://postgres:$${POSTGRES_PASSWORD}@{{ dig .Init.sk "instanceServiceName" "" .Shared }}:5432/postgres"
      labels:
        dev.score.compose.labels.is-init-container: "true"
      depends_on:
        {{ dig .Init.sk "instanceServiceName" "" .Shared }}:
          condition: service_healthy
          restart: true
      volumes:
      - type: bind
        source: {{ .MountsDirectory }}/{{ dig .Init.sk "instanceServiceName" "" .Shared }}-db-scripts
        target: /db-scripts
  info_logs: |
    - "To connect to postgres, enter password {{ .State.password | squote }} at: \"docker run -it --network {{ .ComposeProjectName }}_default --rm postgres:16-alpine psql -h {{ dig .Init.sk "instanceServiceName" "" .Shared }} -U {{ .State.username }} --dbname {{ .State.database }}\""
    {{ if ne .Init.publishPort "0" }}
    - "Or connect your postgres client to \"postgres://{{ .State.username }}:{{ .State.password }}@localhost:{{ .Init.publishPort }}/{{ .State.database }}\""
    {{ end }}

# This resource provides a minio based S3 bucket with AWS-style credentials.
# This provides some common and well known outputs that can be used with any generic AWS s3 client.
# If the provider has a publish port annotation, it can expose a management port on the local network for debugging and
# connectivity.
- uri: template://default-provisioners/s3
  type: s3
  # The init template contains some initial seed data that can be used it needed.
  init: |
    randomServiceName: minio-{{ randAlphaNum 6 }}
    randomUsername: user-{{ randAlpha 8 }}
    randomPassword: {{ randAlphaNum 16 | quote }}
    randomBucket: bucket-{{ randAlpha 8 | lower }}-{{ .Id | lower | trunc 47 }}
    randomAccessKeyId: {{ randAlphaNum 20 | quote }}
    randomSecretKey: {{ randAlphaNum 40 | quote }}
    sk: default-provisioners-minio-instance
    publishPort: {{ dig "annotations" "compose.score.dev/publish-port" "0" .Metadata | atoi }}
  # The only instance state is the bucket name, for now we provision a single aws key across s3 resources.
  state: |
    bucket: {{ dig "bucket" .Init.randomBucket .State | quote }}
  # The shared state contains the chosen service name and credentials
  shared: |
    {{ .Init.sk }}:
      instanceServiceName: {{ dig .Init.sk "instanceServiceName" .Init.randomServiceName .Shared | quote }}
      instanceUsername: {{ dig .Init.sk "instanceUsername" .Init.randomUsername .Shared | quote }}
      instancePassword: {{ dig .Init.sk "instancePassword" .Init.randomPassword .Shared | quote }}
      instanceAccessKeyId: {{ dig .Init.sk "instanceAccessKeyId" .Init.randomAccessKeyId .Shared | quote }}
      instanceSecretKey: {{ dig .Init.sk "instanceSecretKey" .Init.randomSecretKey .Shared | quote }}
      publishPort: {{ with (dig .Init.sk "publishPort" 0 .Shared) }}{{ if ne . 0 }}{{ . }}{{ else }}{{ $.Init.publishPort }}{{ end }}{{ end }}
  # the outputs that we can expose
  outputs: |
    bucket: {{ .State.bucket }}
    access_key_id: {{ dig .Init.sk "instanceAccessKeyId" "" .Shared | quote }}
    secret_key: {{ dig .Init.sk "instanceSecretKey" "" .Shared | quote }}
    endpoint: http://{{ dig .Init.sk "instanceServiceName" "" .Shared }}:9000
    # for compatibility with Humanitec's existing s3 resource
    region: ""
    aws_access_key_id: {{ dig .Init.sk "instanceAccessKeyId" "" .Shared | quote }}
    aws_secret_key: {{ dig .Init.sk "instanceSecretKey" "" .Shared | quote }}
  # we store 2 files, 1 is always the same and overridden, the other is per bucket
  files: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-setup-scripts/00-svcacct.sh: |
      mc alias set myminio http://{{ dig .Init.sk "instanceServiceName" "" .Shared }}:9000 {{ dig .Init.sk "instanceUsername" "" .Shared }} {{ dig .Init.sk "instancePassword" "" .Shared }}
      mc admin user svcacct info myminio {{ dig .Init.sk "instanceAccessKeyId" "" .Shared | quote }} || mc admin user svcacct add myminio {{ dig .Init.sk "instanceUsername" "" .Shared | quote }} --access-key {{ dig .Init.sk "instanceAccessKeyId" "" .Shared | quote }} --secret-key {{ dig .Init.sk "instanceSecretKey" "" .Shared | quote }}
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-setup-scripts/10-bucket-{{ .State.bucket }}.sh: |
      mc alias set myminio http://{{ dig .Init.sk "instanceServiceName" "" .Shared }}:9000 {{ dig .Init.sk "instanceUsername" "" .Shared }} {{ dig .Init.sk "instancePassword" "" .Shared }}
      mc mb -p myminio/{{ .State.bucket }}
  volumes: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-data:
      driver: local
  # 2 services, the minio one, and the init container which ensures the service account and buckets exist
  services: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}:
      image: quay.io/minio/minio
      command: ["server", "/data", "--console-address", ":9001"]
      restart: always
      {{ if ne .Init.publishPort 0 }}
      ports:
      - target: 9001
        published: {{ .Init.publishPort }}
      {{ end }}
      healthcheck:
        test: ["CMD-SHELL", "mc alias set myminio http://localhost:9000 {{ dig .Init.sk "instanceUsername" "" .Shared }} {{ dig .Init.sk "instancePassword" "" .Shared }}"]
        interval: 2s
        timeout: 2s
        retries: 10
      environment:
        MINIO_ROOT_USER: {{ dig .Init.sk "instanceUsername" "" .Shared | quote }}
        MINIO_ROOT_PASSWORD: {{ dig .Init.sk "instancePassword" "" .Shared | quote }}
      volumes:
      - type: volume
        source: {{ dig .Init.sk "instanceServiceName" "" .Shared }}-data
        target: /data
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}-init:
      image: quay.io/minio/minio
      entrypoint: ["/bin/sh"]
      command:
      - "-c"
      - "for s in $$(ls /setup-scripts -1); do sh /setup-scripts/$$s; done"
      labels:
        dev.score.compose.labels.is-init-container: "true"
      depends_on:
        {{ dig .Init.sk "instanceServiceName" "" .Shared }}:
          condition: service_healthy
          restart: true
      volumes:
      - type: bind
        source: {{ .MountsDirectory }}/{{ dig .Init.sk "instanceServiceName" "" .Shared }}-setup-scripts
        target: /setup-scripts
  info_logs: |
    - "To connect with a minio client: use the myminio alias at \"docker run -it --network {{ .ComposeProjectName }}_default --rm --entrypoint /bin/bash quay.io/minio/minio -c 'mc alias set myminio http://{{ dig .Init.sk "instanceServiceName" "" .Shared }}:9000 {{ dig .Init.sk "instanceAccessKeyId" "" .Shared }} {{ dig .Init.sk "instanceSecretKey" "" .Shared }}; bash'\""
    {{ if ne .Init.publishPort 0 }}
    - "Or enter {{ dig .Init.sk "instanceUsername" "" .Shared }} / {{ dig .Init.sk "instancePassword" "" .Shared }} at https://localhost:{{ .Init.publishPort }}"
    {{ end }}

# The default dns provisioner just outputs localhost as the hostname every time.
# This is because without actual control of a dns resolver we can't do any accurate routing on any other name. This
# can be replaced by a new provisioner in the future.
- uri: template://default-provisioners/dns
  type: dns
  class: default
  outputs: |
    host: localhost

# The default route provisioner sets up an nginx service with an HTTP service that can route on our prefix paths.
# It assumes the hostnames and routes provided have no overlaps. Weird behavior may happen if there are overlaps.
- uri: template://default-provisioners/route
  type: route
  class: default
  init: |
    randomServiceName: routing-{{ randAlphaNum 6 }}
    sk: default-provisioners-routing-instance
    {{ if not (regexMatch "^/|(/([^/]+))+$" .Params.path) }}{{ fail "params.path start with a / but cannot end with /" }}{{ end }}
    {{ if not (regexMatch "^[a-z0-9_.-]{1,253}$" .Params.host) }}{{ fail "params.host must be a valid hostname" }}{{ end }}
    {{ $ports := (index .WorkloadServices .SourceWorkload).Ports }}
    {{ if not $ports }}{{ fail "no service ports exist" }}{{ end }}
    {{ $port := index $ports (print .Params.port) }}
    {{ if not $port.TargetPort }}{{ fail "params.port is not a named service port" }}{{ end }}
  shared: |
    {{ .Init.sk }}:
      instancePort: 8080
      instanceServiceName: {{ dig .Init.sk "instanceServiceName" .Init.randomServiceName .Shared | quote }}
      {{ $targetHost := (index .WorkloadServices .SourceWorkload).ServiceName }}
      {{ $ports := (index .WorkloadServices .SourceWorkload).Ports }}
      {{ $port := index $ports (print .Params.port) }}
      {{ $targetPort := $port.TargetPort }}
      {{ $target := (printf "%s:%d" $targetHost $targetPort) }}
      {{ $hBefore := dig .Init.sk "hosts" (dict) .Shared }}
      {{ $rBefore := dig .Params.host (dict) $hBefore }}
      {{ $inner := dict "path" .Params.path "target" $target "port" $targetPort }}
      {{ $rAfter := (merge $rBefore (dict .Uid $inner)) }}
      {{ $hAfter := (merge $hBefore (dict .Params.host $rAfter)) }}
      hosts: {{ $hAfter | toRawJson }}
  files: |
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}/nginx.conf: |
      worker_processes 1;
      worker_rlimit_nofile 8192;
      events {
        worker_connections 4096;
      }
      http {
        resolver 127.0.0.11;
    
        {{ range $h, $r := (dig .Init.sk "hosts" "" .Shared) }}
        server {
          listen 80;
          listen [::]:80;
          server_name {{ $h }};
    
          proxy_set_header X-Real-IP              $remote_addr;
          proxy_set_header X-Forwarded-For        $remote_addr;
          proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;
          proxy_set_header Proxy                  "";
          proxy_connect_timeout                   5s;
          proxy_send_timeout                      60s;
          proxy_read_timeout                      60s;
          proxy_buffers                           16 4k;
          proxy_buffer_size                       2k;
          
          location = /favicon.ico {
            return 204;
            access_log     off;
            log_not_found  off;
          }
          
          {{ range $k, $v := $r }}
          location = {{ index $v "path" }} {
            set $backend {{ index $v "target" }};
            rewrite ^{{ index $v "path" }}(.*)$ /$1 break;
            proxy_pass http://$backend;
          }

          {{ if not (eq (index $v "path") "/") }}
          location {{ index $v "path" }}/ {
            set $backend {{ index $v "target" }};
            rewrite ^{{ index $v "path" }}/(.*)$ /$1 break;
            proxy_pass http://$backend;
          }
          {{ end }}
          {{ end }}
        }
        {{ end }}
      }

  services: |
    {{ $p := (dig .Init.sk "instancePort" 0 .Shared) }}
    {{ dig .Init.sk "instanceServiceName" "" .Shared }}:
      image: "nginx:1"
      restart: always
      ports:
        - published: {{ $p }}
          target: 80
      volumes:
        - type: bind
          source: {{ .MountsDirectory }}/{{ dig .Init.sk "instanceServiceName" "" .Shared }}/nginx.conf
          target: /etc/nginx/nginx.conf
          readOnly: true
  info_logs: |
    {{ $p := (dig .Init.sk "instancePort" 0 .Shared) }}
    - "{{.Uid}}: To connect to this route, http://{{ .Params.host }}:{{ $p }}{{ .Params.path }} (make sure {{ .Params.host }} resolves to localhost)"
